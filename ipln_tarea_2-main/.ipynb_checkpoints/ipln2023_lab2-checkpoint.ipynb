{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4azOYi8KSoC"
   },
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural\n",
    "\n",
    "# Tarea 2\n",
    "\n",
    "El objetivo de este laboratorio es realizar diferentes experimentos para representar y clasificar textos. Para esto se trabajará con un corpus para análisis de sentimiento, creado para la competencia [TASS 2020](http://www.sepln.org/workshops/tass/) (IberLEF - SEPLN).\n",
    "\n",
    "### Entrega\n",
    "Deberán entregar un archivo *.ipynb* con su solución, que incluya código, comentarios y respuestas a las preguntas que se incluyen al final de este notebook. \n",
    "\n",
    "El plazo de entrega de la tarea 2 cierra el **20 de junio a las 23:59 horas**.\n",
    "\n",
    "### Plataforma sugerida\n",
    "Sugerimos que utilicen la plataforma [Google colab](https://colab.research.google.com/), que permite trabajar colaborativamente con un *notebook* de python. Al finalizar pueden descargar ese *notebook* en un archivo .ipynb, incluyendo las salidas ya ejecutadas, con la opción ```File -> Download -> Download .ipynb```.\n",
    "\n",
    "### Aprobación del laboratorio\n",
    "Para aprobar el laboratorio se exige como mínimo: \n",
    "* Probar dos enfoques diferentes para la representación de tweets (uno basado en BoW y otro en word embeddings)\n",
    "* Probar al menos dos modelos de aprendizaje automático con cada representación\n",
    "* Comparar los resultados con los obtenidos por el modelo de pysentimiento. \n",
    "El preprocesamiento, las pruebas con otras formas de representación de los tweets, los experimentos con otros modelos de aprendizaje automático, incluyendo aprendizaje profundo, entre otros posibles experimentos, no son requisito para aprobar el laboratorio, aunque aportan a la nota final.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAumcYFLP0f8"
   },
   "source": [
    "## Parte 1 - Carga y preprocesamiento del corpus\n",
    "\n",
    "Para trabajar en este notebook deben cargar los tres archivos disponbiles en eva: train.csv, devel.csv y test.csv.\n",
    "\n",
    "La aplicación de una etapa de preprocesamiento similar a la implementada en la tarea 1 es opcional. Es interesante hacer experimentos con y sin la etapa de preprocesamiento, de modo de comparar resultados (sobre el corpus de desarrollo, devel.csv) y definir si se incluye o no en la solución final.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nnBJGtH5QLcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded\n"
     ]
    }
   ],
   "source": [
    "import code.carga as carga\n",
    "\n",
    "DATASET_PATH = './csv/'\n",
    "\n",
    "\n",
    "# Carga de los datasets\n",
    "devel_df, train_df, test_df = carga.get_datasets(dataset_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T2kOeMA_ly_b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet original:\n",
      "@pvaya @Sailor_Tesserei @Sh3rryMH es que es una reivindicación muy de moda... Ya sabes... de gente pedante como yo \n",
      "Tweet preprocesado:\n",
      "Preprocesamiento DummyPreprocess\n",
      "@pvaya @sailor_tesserei @sh3rrymh es que es una reivindicación muy de moda... ya sabes... de gente pedante como yo \n",
      "\n",
      "Preprocesamiento AllPreprocess\n",
      "mención mención mención reivindicación muy moda sabes gente pedante\n",
      "\n",
      "Preprocesamiento StopWordsPreprocess\n",
      "pvaya sailor_tesserei sh3rrymh reivindicación muy moda sabes gente pedante\n",
      "\n",
      "Preprocesamiento Tarea1Preprocess\n",
      "mención mención mención es que es una reivindicación muy de moda ya sabes de gente pedante como yo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de los tweets\n",
    "import code.preprocessing as pre\n",
    "\n",
    "preprocessing_methods = [pre.DummyPreprocess(), pre.AllPreprocess(), pre.StopWordsPreprocess(), pre.Tarea1Preprocess()]\n",
    "pre.set_abbr_dir('./csv/abbreviations/')\n",
    "pre.set_lexico_path('./csv/')\n",
    "\n",
    "\n",
    "tweet = train_df.sample(n=1,random_state=6).iloc[0]['Tweet'] # seed 2 ejemplo simple\n",
    "print('\\nTweet original:')\n",
    "print(tweet)\n",
    "print('Tweet preprocesado:')\n",
    "for method in preprocessing_methods:\n",
    "    print('Preprocesamiento', method.__class__.__name__)\n",
    "    print(method.preprocess_tweet(tweet))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZMlbsw2uFLm"
   },
   "source": [
    "## Parte 2 - Representación de los tweets\n",
    "\n",
    "Para representar los tweets se pide que experimenten con modelos basados en Bag of Words (BoW) y con Word Embeddings.\n",
    "\n",
    "Para los dos enfoques podrán elegir entre diferentes opciones:\n",
    "\n",
    "**Bag of Words**\n",
    "\n",
    "* BOW estándar: se recomienda trabajar con la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de sklearn, en particular, fit_transform y transform.\n",
    "* BOW filtrando stop-words: tienen disponible en eva una lista de stop-words para el español, adaptada para análisis de sentimiento (no se filtran palabras relevantes para determinar la polaridad, como \"no\", \"pero\", etc.).\n",
    "* BoW usando lemas: pueden usar herramientas de spacy.\n",
    "* BOW seleccionando las features más relevantes: se recomienda usar la clase [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html?highlight=select%20k%20best#sklearn.feature_selection.SelectKBest) y probar con diferentes valores de k (por ejemplo, 10, 50, 200, 1000).\n",
    "* BOW combinado con TF-IDF: se recomienda usar la clase [TfidfVectorizer](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "**Word Embeddings**\n",
    "\n",
    "* A partir de los word embeddings, representar cada tweet como el vector promedio (mean vector) de los vectores de las palabras que lo componen.\n",
    "* A partir de los word embeddings, representar cada tweet como la concatenación de los vectores de las palabras que lo componen (llevando el vector total a un largo fijo).\n",
    "\n",
    "Se recomienda trabajar con alguna de las colecciones de word embeddings disponibles en https://github.com/dccuchile/spanish-word-embeddings. El repositorio incluye links a ejemplos y tutoriales.\n",
    "\n",
    "\n",
    "Se pide que prueben al menos una opción basada en BoW y una basada en word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G9BNltLduHqB"
   },
   "outputs": [],
   "source": [
    "# Representación de los tweets usando BoW\n",
    "import code.representacion as rep\n",
    "\n",
    "standard_bow = rep.StandardBowMapper()\n",
    "count_bow = rep.CountStandardBowMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZFyYXHmUWHr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings model...\n",
      "Embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "# Representación de los tweets usando word embeddings\n",
    "import code.representacion as rep\n",
    "import code.test as test\n",
    "\n",
    "EMBEDDING_PATH = './embeddings/embeddings-l-model.vec'\n",
    "\n",
    "mean_tweet_lenght = test.get_mean_tweet_lenght(train_df)\n",
    "wordvectors = carga.get_embeddings(word_vector_path = EMBEDDING_PATH, limit=300000)\n",
    "mean_embedding = rep.MeanEmbeddingMapper(wordvectors)\n",
    "array_embedding = rep.ArrayEmbeddingMapper(mean_tweet_lenght, wordvectors)\n",
    "count_mean_embedding = rep.CountMeanEmbeddingMapper(wordvectors)\n",
    "count_array_embedding = rep.CountArrayEmbeddingMapper(mean_tweet_lenght, wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxAcPhqgWHr2"
   },
   "source": [
    "## Parte 3 - Clasificación de los tweets\n",
    "\n",
    "Para la clasificación de los tweets es posible trabajar con dos enfoques diferentes:\n",
    "\n",
    "* Aprendizaje Automático basado en atributos: se pide probar al menos dos modelos diferentes, por ejemplo, Multi Layer Perceptron ([MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)) y Support Vector Machines ([SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)), y usar al menos dos formas de representación de tweets (una basada en BoW y otra basada en word embeddings). Se publicó en eva un léxico de palabras positivas y negativas que puede ser utilizado para generar atributos.\n",
    "\n",
    "* Aprendizaje Profundo: se recomienda experimentar con alguna red recurrente como LSTM. En este caso deben representar los tweets an base a word embeddings.\n",
    "\n",
    "Deberán usar el corpus de desarrollo (devel.csv) para comparar resultados de diferentes experimentos, variando los valores de los hiperparámetros, la forma de representación de los tweets, el preprocesamiento, los modelos de AA, etc. \n",
    "\n",
    "Tanto para la evaluación sobre desarrollo como para la evaluación final sobre test se usará la medida [Macro-F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) (promedio de la medida F1 de cada clase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import code.clasificacion as clf\n",
    "import code.test as test\n",
    "import time\n",
    "import code.load_run_data as load\n",
    "from IPython.display import clear_output\n",
    "\n",
    "svm_parameters = {'c':[1, 2], 'kernel':['rbf', 'poly']}\n",
    "knn_parameters ={'n_neighbors':[5, 3, 7, 10], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
    "mlp_embedding_parameters = {'max_iter':[100, 200, 400], 'hidden_layer_sizes':[(50,), (100,), (50, 50), (100, 100)],\n",
    "                             'activation':['relu', 'logistic'], 'random_state':[1]}\n",
    "mlp_bow_parameters = {'max_iter':[400], 'hidden_layer_sizes':[(100,), (50, 50)], 'activation':['relu', 'logistic'], 'random_state':[1]}\n",
    "lstm_parameters_embedding = {'sequence_length': [mean_tweet_lenght], 'embedding_length': [300], 'lstm_layers': [[(50,'tanh','sigmoid')]],\n",
    "                        'dense_layer': [[(50,None)], [(50,'relu')], [(100, 'relu')], [(100,'relu'), (100,'relu'), (100,'relu')]], 'seed': [1]}\n",
    "lstm_parameters_count_embedding = {'sequence_length': [mean_tweet_lenght], 'embedding_length': [303], 'lstm_layers': [[(50,'tanh','sigmoid')]],\n",
    "                        'dense_layer': [[(50,None)], [(50,'relu')], [(100, 'relu')], [(100,'relu'), (100,'relu'), (100,'relu')]], 'seed': [1]}\n",
    "\n",
    "\n",
    "time_id = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos con Aprendizaje Automatico y BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_svm_bow_results.csv\n",
      "Top 3 results:\n",
      "   Classifier    Metaparameters    Word Representation    Preprocessing  F1 Score  Train Time  Test time\n",
      "SVMClassifier c: 2, kernel: rbf CountStandardBowMapper    AllPreprocess  0.604917    8.626708   0.708100\n",
      "SVMClassifier c: 2, kernel: rbf CountStandardBowMapper Tarea1Preprocess  0.592301   10.813308   0.951350\n",
      "SVMClassifier c: 2, kernel: rbf CountStandardBowMapper  DummyPreprocess  0.588521   12.266164   1.175033\n"
     ]
    }
   ],
   "source": [
    "classifiers_svm = test.get_classifiers(svm_parameters, clf.SVMClassifier)\n",
    "results_classifiers_svm_bow = test.test_classifers(classifiers_svm, [standard_bow, count_bow], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_classifiers_svm_bow, \"svm_bow\", time_id, './results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_knn_bow_results.csv\n",
      "Top 3 results:\n",
      "   Classifier                           Metaparameters    Word Representation       Preprocessing  F1 Score  Train Time  Test time\n",
      "KNNClassifier  n_neighbors: 10, weights: uniform, p: 1 CountStandardBowMapper StopWordsPreprocess  0.554032    0.005330   0.489399\n",
      "KNNClassifier  n_neighbors: 7, weights: distance, p: 1 CountStandardBowMapper StopWordsPreprocess  0.552499    0.004000   0.461447\n",
      "KNNClassifier n_neighbors: 10, weights: distance, p: 1 CountStandardBowMapper StopWordsPreprocess  0.551799    0.004965   0.470813\n"
     ]
    }
   ],
   "source": [
    "classifiers_knn = test.get_classifiers(knn_parameters, clf.KNNClassifier)\n",
    "results_classifiers_knn_bow = test.test_classifers(classifiers_knn, [standard_bow, count_bow], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_classifiers_knn_bow, \"knn_bow\", time_id, './results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_mlp_for_bow_results.csv\n",
      "Top 3 results:\n",
      "   Classifier                                                                 Metaparameters    Word Representation       Preprocessing  F1 Score  Train Time  Test time\n",
      "MLPClassifier max_iter: 400, hidden_layer_sizes: (50, 50), activation: relu, random_state: 1 CountStandardBowMapper StopWordsPreprocess  0.576701  119.607115   0.002924\n",
      "MLPClassifier   max_iter: 400, hidden_layer_sizes: (100,), activation: relu, random_state: 1 CountStandardBowMapper StopWordsPreprocess  0.576433  320.371513   0.002924\n",
      "MLPClassifier max_iter: 400, hidden_layer_sizes: (50, 50), activation: relu, random_state: 1 CountStandardBowMapper       AllPreprocess  0.575675   97.333843   0.002001\n"
     ]
    }
   ],
   "source": [
    "mlp_for_bow_classifiers = test.get_classifiers(mlp_bow_parameters, clf.MLPClassifier)  \n",
    "results_mlp_for_bow = test.test_classifers(mlp_for_bow_classifiers, [standard_bow, count_bow], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_mlp_for_bow, \"mlp_for_bow\", time_id, './results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos con Aprendizaje Automatico y Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_svm_emb_results.csv\n",
      "Top 3 results:\n",
      "   Classifier    Metaparameters Word Representation       Preprocessing  F1 Score  Train Time  Test time\n",
      "SVMClassifier c: 1, kernel: rbf MeanEmbeddingMapper       AllPreprocess  0.634167   11.123228   2.173713\n",
      "SVMClassifier c: 2, kernel: rbf MeanEmbeddingMapper       AllPreprocess  0.630164   11.405119   2.338367\n",
      "SVMClassifier c: 1, kernel: rbf MeanEmbeddingMapper StopWordsPreprocess  0.629565   10.784764   2.101844\n"
     ]
    }
   ],
   "source": [
    "classifiers_svm = test.get_classifiers(svm_parameters, clf.SVMClassifier)\n",
    "results_classifiers_svm_emb = test.test_classifers(classifiers_svm, [mean_embedding, count_mean_embedding], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_classifiers_svm_emb, \"svm_emb\", time_id, './results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_knn_emb_results.csv\n",
      "Top 3 results:\n",
      "   Classifier                          Metaparameters      Word Representation Preprocessing  F1 Score  Train Time  Test time\n",
      "KNNClassifier  n_neighbors: 5, weights: uniform, p: 1 CountMeanEmbeddingMapper AllPreprocess  0.570104    0.007999   0.516565\n",
      "KNNClassifier n_neighbors: 10, weights: uniform, p: 1 CountMeanEmbeddingMapper AllPreprocess  0.563895    0.008502   0.583435\n",
      "KNNClassifier n_neighbors: 5, weights: distance, p: 1 CountMeanEmbeddingMapper AllPreprocess  0.562804    0.009000   0.479562\n"
     ]
    }
   ],
   "source": [
    "classifiers_knn = test.get_classifiers(knn_parameters, clf.KNNClassifier)\n",
    "results_classifiers_knn_emb = test.test_classifers(classifiers_knn, [mean_embedding, count_mean_embedding], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_classifiers_knn_emb, \"knn_emb\", time_id, './results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_mlp_for_embedding_results.csv\n",
      "Top 3 results:\n",
      "   Classifier                                                                   Metaparameters      Word Representation       Preprocessing  F1 Score  Train Time  Test time\n",
      "MLPClassifier max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1 CountMeanEmbeddingMapper       AllPreprocess  0.637245   11.696102   0.007427\n",
      "MLPClassifier max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1 CountMeanEmbeddingMapper StopWordsPreprocess  0.633987   11.479834   0.008090\n",
      "MLPClassifier  max_iter: 100, hidden_layer_sizes: (50,), activation: logistic, random_state: 1 CountMeanEmbeddingMapper StopWordsPreprocess  0.633377    7.899633   0.005014\n"
     ]
    }
   ],
   "source": [
    "mlp_for_embedding_classifiers = test.get_classifiers(mlp_embedding_parameters, clf.MLPClassifier)\n",
    "results_mlp_for_emb = test.test_classifers(mlp_for_embedding_classifiers, [mean_embedding, count_mean_embedding], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_mlp_for_emb, \"mlp_for_embedding\", time_id, './results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos con Aprendizaje Profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largo de secuencia 300:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_lstm_sl_300_results.csv\n",
      "Top 3 results:\n",
      "    Classifier                                                                                                            Metaparameters  Word Representation       Preprocessing  F1 Score  Train Time  Test time\n",
      "LSTMClassifier sequence_length: 9, embedding_length: 300, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1 ArrayEmbeddingMapper StopWordsPreprocess  0.586923   25.574628   0.195253\n",
      "LSTMClassifier    sequence_length: 9, embedding_length: 300, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(50, None)], seed: 1 ArrayEmbeddingMapper StopWordsPreprocess  0.582857   25.887776   0.172518\n",
      "LSTMClassifier  sequence_length: 9, embedding_length: 300, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(50, 'relu')], seed: 1 ArrayEmbeddingMapper    Tarea1Preprocess  0.582669   25.315014   0.173689\n"
     ]
    }
   ],
   "source": [
    "lstm_classifiers = test.get_classifiers(lstm_parameters_embedding, clf.LSTMClassifier)\n",
    "results_lstm_sl_300 = test.test_classifers(lstm_classifiers, [array_embedding], preprocessing_methods, train_df, devel_df) \n",
    "clear_output()\n",
    "test.dump_results(results_lstm_sl_300, \"lstm_sl_300\", time_id, './results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largo de secuencia 303:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful run\n",
      "Results saved in ./results/1687138584_lstm_sl_303_results.csv\n",
      "Top 3 results:\n",
      "    Classifier                                                                                                            Metaparameters       Word Representation       Preprocessing  F1 Score  Train Time  Test time\n",
      "LSTMClassifier sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1 CountArrayEmbeddingMapper StopWordsPreprocess  0.600652   25.310352   0.163551\n",
      "LSTMClassifier sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1 CountArrayEmbeddingMapper    Tarea1Preprocess  0.596711   25.244154   0.154698\n",
      "LSTMClassifier sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1 CountArrayEmbeddingMapper       AllPreprocess  0.592267   25.533471   0.158754\n"
     ]
    }
   ],
   "source": [
    "lstm_classifiers_count_embedding = test.get_classifiers(lstm_parameters_count_embedding, clf.LSTMClassifier)\n",
    "results_lstm_sl_303 = test.test_classifers(lstm_classifiers_count_embedding, [count_array_embedding], preprocessing_methods, train_df, devel_df)\n",
    "clear_output()\n",
    "test.dump_results(results_lstm_sl_303, \"lstm_sl_303\", time_id, './results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de todos los experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo a continuacion resume los resultados obtenidos anteriormente, mostrando los mejores resultados obtenidos para cada clasificador, y un ranking global de los mejores 10 configuraciones de clasificacadores/representaciones/preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10:\n",
      "   Classifier      Word Representation       Preprocessing  F1 Score  Train Time  Test time                                                                       Metaparameters\n",
      "MLPClassifier CountMeanEmbeddingMapper       AllPreprocess  0.637245   11.696102   0.007427   {max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
      "SVMClassifier      MeanEmbeddingMapper       AllPreprocess  0.634167   11.123228   2.173713                                                                  {c: 1, kernel: rbf}\n",
      "MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.633987   11.479834   0.008090   {max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
      "MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.633377    7.899633   0.005014    {max_iter: 100, hidden_layer_sizes: (50,), activation: logistic, random_state: 1}\n",
      "SVMClassifier      MeanEmbeddingMapper       AllPreprocess  0.630164   11.405119   2.338367                                                                  {c: 2, kernel: rbf}\n",
      "MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.629970   21.811296   0.007003   {max_iter: 200, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
      "MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.629750   11.970838   0.006922 {max_iter: 100, hidden_layer_sizes: (50, 50), activation: logistic, random_state: 1}\n",
      "SVMClassifier      MeanEmbeddingMapper StopWordsPreprocess  0.629565   10.784764   2.101844                                                                  {c: 1, kernel: rbf}\n",
      "MLPClassifier CountMeanEmbeddingMapper       AllPreprocess  0.629468   12.131259   0.008550 {max_iter: 100, hidden_layer_sizes: (50, 50), activation: logistic, random_state: 1}\n",
      "MLPClassifier CountMeanEmbeddingMapper       AllPreprocess  0.628936   22.438816   0.007017   {max_iter: 200, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n"
     ]
    }
   ],
   "source": [
    "import code.load_run_data as load\n",
    "\n",
    "load.print_global_summary(path='./results', global_top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMENTARIO DE POR CLASIFICADOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 per classifier:\n",
      "\n",
      "Classifier: KNNClassifier \n",
      "   Classifier      Word Representation Preprocessing  F1 Score  Train Time  Test time                            Metaparameters\n",
      "KNNClassifier CountMeanEmbeddingMapper AllPreprocess  0.570104    0.007999   0.516565  {n_neighbors: 5, weights: uniform, p: 1}\n",
      "KNNClassifier CountMeanEmbeddingMapper AllPreprocess  0.563895    0.008502   0.583435 {n_neighbors: 10, weights: uniform, p: 1}\n",
      "KNNClassifier CountMeanEmbeddingMapper AllPreprocess  0.562804    0.009000   0.479562 {n_neighbors: 5, weights: distance, p: 1}\n",
      "\n",
      "Classifier: LSTMClassifier \n",
      "    Classifier       Word Representation       Preprocessing  F1 Score  Train Time  Test time                                                                                                              Metaparameters\n",
      "LSTMClassifier CountArrayEmbeddingMapper StopWordsPreprocess  0.600652   25.310352   0.163551 {sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1}\n",
      "LSTMClassifier CountArrayEmbeddingMapper    Tarea1Preprocess  0.596711   25.244154   0.154698 {sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1}\n",
      "LSTMClassifier CountArrayEmbeddingMapper       AllPreprocess  0.592267   25.533471   0.158754 {sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1}\n",
      "\n",
      "Classifier: MLPClassifier \n",
      "   Classifier      Word Representation       Preprocessing  F1 Score  Train Time  Test time                                                                     Metaparameters\n",
      "MLPClassifier CountMeanEmbeddingMapper       AllPreprocess  0.637245   11.696102   0.007427 {max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
      "MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.633987   11.479834   0.008090 {max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
      "MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.633377    7.899633   0.005014  {max_iter: 100, hidden_layer_sizes: (50,), activation: logistic, random_state: 1}\n",
      "\n",
      "Classifier: SVMClassifier \n",
      "   Classifier Word Representation       Preprocessing  F1 Score  Train Time  Test time      Metaparameters\n",
      "SVMClassifier MeanEmbeddingMapper       AllPreprocess  0.634167   11.123228   2.173713 {c: 1, kernel: rbf}\n",
      "SVMClassifier MeanEmbeddingMapper       AllPreprocess  0.630164   11.405119   2.338367 {c: 2, kernel: rbf}\n",
      "SVMClassifier MeanEmbeddingMapper StopWordsPreprocess  0.629565   10.784764   2.101844 {c: 1, kernel: rbf}\n"
     ]
    }
   ],
   "source": [
    "import code.load_run_data as load\n",
    "\n",
    "load.print_algorithm_summary(path='./results', top_per_classifier=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMENTARIO DE GLOBAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6VGFrEoWHr3"
   },
   "source": [
    "## Parte 4: Evaluación sobre test\n",
    "\n",
    "Deben probar los mejores modelos obtenidos en la parte anterior sobre el corpus de test.\n",
    "\n",
    "También deben comparar sus resultados con un modelo pre-entrenado para análisis de sentimientos de la biblioteca [pysentimiento](https://github.com/pysentimiento/pysentimiento) (deben aplicarlo sobre el corpus de test).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OzTMYHtsq5S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import code.clasificacion as clf\n",
    "import code.test as test\n",
    "import code.pysentimientod as ps\n",
    "\n",
    "# Evaluación sobre test\n",
    "new_train_df = pd.concat([train_df, devel_df], ignore_index=True)\n",
    "\n",
    "summarize = pd.DataFrame(columns=['Classifier', 'Metaparameters', 'Word Representation', 'Preprocessing', 'F1 Score',\n",
    "                                       'Train Time', 'Test time'])\n",
    "\n",
    "top_4_configs = [[clf.MLPClassifier([('max_iter',100), ('hidden_layer_sizes',100), ('activation', 'logistic'), ('random_state', 1)]), count_mean_embedding, preprocessing_methods[1]],\n",
    " [clf.SVMClassifier([('c',1), ('kernel','rbf')]), mean_embedding, preprocessing_methods[1]],\n",
    " [clf.MLPClassifier([('max_iter',100), ('hidden_layer_sizes',100), ('activation', 'logistic'), ('random_state', 1)]), count_mean_embedding, preprocessing_methods[2]],\n",
    " [clf.MLPClassifier([('max_iter',100), ('hidden_layer_sizes',50), ('activation', 'logistic'), ('random_state', 1)]), count_mean_embedding, preprocessing_methods[2]]]\n",
    "\n",
    "#KNNClassifier CountMeanEmbeddingMapper AllPreprocess  0.570104    0.007999   0.516565  {n_neighbors: 5, weights: uniform, p: 1}\n",
    "#KNNClassifier CountMeanEmbeddingMapper AllPreprocess  0.563895    0.008502   0.583435 {n_neighbors: 10, weights: uniform, p: 1}\n",
    "#LSTMClassifier CountArrayEmbeddingMapper StopWordsPreprocess  0.600652   25.310352   0.163551 {sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1}\n",
    "#LSTMClassifier CountArrayEmbeddingMapper    Tarea1Preprocess  0.596711   25.244154   0.154698 {sequence_length: 9, embedding_length: 303, lstm_layers: [(50, 'tanh', 'sigmoid')], dense_layers: [(100, 'relu')], seed: 1}\n",
    "#MLPClassifier CountMeanEmbeddingMapper       AllPreprocess  0.637245   11.696102   0.007427 {max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
    "#MLPClassifier CountMeanEmbeddingMapper StopWordsPreprocess  0.633987   11.479834   0.008090 {max_iter: 100, hidden_layer_sizes: (100,), activation: logistic, random_state: 1}\n",
    "\n",
    "#---SVMClassifier MeanEmbeddingMapper       AllPreprocess  0.634167   11.123228   2.173713 {c: 1, kernel: rbf}\n",
    "#---SVMClassifier MeanEmbeddingMapper       AllPreprocess  0.630164   11.405119   2.338367 {c: 2, kernel: rbf}\n",
    "\n",
    "preprocessing_methods = [pre.DummyPreprocess(), pre.AllPreprocess(), pre.StopWordsPreprocess(), pre.Tarea1Preprocess()]\n",
    "\n",
    "best_two_configs_per_clasiffier = [[clf.SVMClassifier([('c',1), ('kernel','rbf')]), mean_embedding, preprocessing_methods[1]],\n",
    " [clf.SVMClassifier([('c',2), ('kernel','rbf')]), mean_embedding, preprocessing_methods[1]],\n",
    "                                   \n",
    " [clf.MLPClassifier([('c',1), ('kernel','rbf')]), standard_bow, preprocessing_methods[2]],\n",
    " [clf.SVMClassifier([('c',1), ('kernel','rbf')]), standard_bow, preprocessing_methods[2]]]\n",
    "\n",
    "for classifier, word_rep, preprocesing in best_classifiers:\n",
    "    f1_score, train_time, test_time = test.test_single_classifer(classifier, word_rep, preprocesing, new_train_df, test_df)\n",
    "    row = [classifier.__class__.__name__, classifier.get_metaparameters(),\n",
    "           word_rep.__class__.__name__, preprocesing.__class__.__name__,\n",
    "           f1_score, train_time, test_time]\n",
    "    summarize.loc[len(summarize.index)] = row\n",
    "\n",
    "X_test, Y_test = test.split_dataset(test_df)\n",
    "pysentimiento_f1_score = ps.test_pysentimiento(X_test, Y_test)\n",
    "\n",
    "row = ['pysentimiento', '-', '-', '-', pysentimiento_f1_score, '-', '-']\n",
    "summarize.loc[len(summarize.index)] = row\n",
    "\n",
    "summarize = summarize.sort_values(by=[\"F1 Score\"], ascending=False)\n",
    "print(summarize.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn-TGtvvst3T"
   },
   "source": [
    "## Preguntas finales\n",
    "\n",
    "Responda las siguientes preguntas:\n",
    "\n",
    "1) ¿Qué modelos probaron para la representación de los tweets?\n",
    "\n",
    "2) ¿Aplicaron algún tipo de preprocesamiento de los textos?\n",
    "\n",
    "3) ¿Qué modelos de aprendizaje automático probaron?\n",
    "\n",
    "4) ¿Qué atributos utilizaron para estos modelos?\n",
    "\n",
    "5) ¿Probaron algún enfoque de aprendizaje profundo?\n",
    "\n",
    "6) ¿Probaron diferentes configuraciones de hiperparámetros?\n",
    "\n",
    "7) ¿Qué enfoque (preprocesamiento + representación de tweets + modelo + atributos/parámetros) obtuvo la mejor Macro-F1?\n",
    "\n",
    "8) ¿Qué clase es la mejor clasificada por este enfoque? ¿Cuál es la peor? ¿Por qué piensan que sucede esto?\n",
    "\n",
    "9) ¿Cómo son sus resultados en comparación con los de pysentimiento? ¿Por qué piensan que sucede esto?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
